{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the 1D CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import expressyeaself.construct_neural_net as construct\n",
    "import expressyeaself.encode_sequences as encode\n",
    "import expressyeaself.organize_data as organize \n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import subprocess\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "# tf.get_variable('test_bool', 1, tf.bool)\n",
    "from tensorflow.python.keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.layers import (Add, Concatenate, Input, Dense, \n",
    "                                            Dropout, Embedding, Conv1D, \n",
    "                                            MaxPooling1D, GlobalAveragePooling1D, \n",
    "                                            Flatten)\n",
    "from tensorflow.python.keras import regularizers\n",
    "from tensorflow.python.keras.utils import to_categorical\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "ROOT_DIR = os.getcwd() + '/'\n",
    "CHECKPOINTS_DIR = ROOT_DIR + 'expressyeaself/models/1dcnn/checkpoints/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the full data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_filename = ('20190612130111781831_percentiles_els_binarized_homogeneous'\n",
    "                   '_deflanked_sequences_with_exp_levels.txt.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a smaller sample set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_filename = '10000_from_' + sample_filename "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the absolute path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = ROOT_DIR + 'example/processed_data/' + sample_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1355481147766113\n"
     ]
    }
   ],
   "source": [
    "import time as t\n",
    "t0 = t.time()\n",
    "X_padded, y_scaled, abs_max_el = encode.encode_sequences_with_method(sample_path, method='One-Hot', scale_els=True, model_type='1DCNN', binarized_els=True)\n",
    "num_seqs, max_sequence_len = organize.get_num_and_len_of_seqs_from_file(sample_path)\n",
    "t1 = t.time()\n",
    "print(t1-t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape expression levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scaled = y_scaled.reshape((len(y_scaled), 1))\n",
    "# scaler = MinMaxScaler()\n",
    "# scaler.fit(y_scaled)\n",
    "# y_scaled = scaler.transform(y_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform a train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_scaled, test_size=test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0612 13:29:52.200875 4496975296 callbacks.py:1466] `write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n",
      "W0612 13:29:52.201920 4496975296 callbacks.py:1469] `batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 78, 15)            240       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 76, 15)            690       \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 74, 15)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 72, 15)            690       \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 70, 15)            690       \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 16        \n",
      "=================================================================\n",
      "Total params: 2,326\n",
      "Trainable params: 2,326\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the model parameters\n",
    "batch_size = len(y_scaled) * 0.01 # no bigger than 1 % of data\n",
    "filters = 15\n",
    "kernel_size = 3\n",
    "strides = 1\n",
    "epochs = 20\n",
    "dropout = 0.5\n",
    "\n",
    "# Define the tensorboard and checkpointer if desired\n",
    "tb = TensorBoard(log_dir='./logs', \n",
    "                 histogram_freq=3, \n",
    "                 batch_size=batch_size, \n",
    "                 write_graph=True, \n",
    "                 write_grads=True, \n",
    "                 write_images=True)\n",
    "checkpointer = ModelCheckpoint(monitor='val_acc', \n",
    "                               filepath=(CHECKPOINTS_DIR + '1dcnn_onehot.hdf5'), \n",
    "                               verbose=1, \n",
    "                               save_best_only=True)\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Build up the layers\n",
    "model.add(Conv1D(filters, kernel_size, activation='relu', \n",
    "                 input_shape=(max_sequence_len, 5), \n",
    "                 kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "model.add(MaxPooling1D(3, strides))\n",
    "#     keras.layers.Flatten(data_format=None)\n",
    "# model.add(GlobalAveragePooling1D())\n",
    "model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "\n",
    "# Add some dense and dropout layers\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mse', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0612 13:29:52.417435 4496975296 callbacks.py:1466] `write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n",
      "W0612 13:29:52.418480 4496975296 callbacks.py:1469] `batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 80, 5)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 80, 15)       90          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 78, 15)       240         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 76, 15)       390         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 74, 15)       540         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 72, 15)       690         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 70, 15)       840         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 68, 15)       990         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 66, 15)       1140        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 64, 15)       1290        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 62, 15)       1440        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 710, 15)      0           conv1d_4[0][0]                   \n",
      "                                                                 conv1d_5[0][0]                   \n",
      "                                                                 conv1d_6[0][0]                   \n",
      "                                                                 conv1d_7[0][0]                   \n",
      "                                                                 conv1d_8[0][0]                   \n",
      "                                                                 conv1d_9[0][0]                   \n",
      "                                                                 conv1d_10[0][0]                  \n",
      "                                                                 conv1d_11[0][0]                  \n",
      "                                                                 conv1d_12[0][0]                  \n",
      "                                                                 conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 10650)        0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 500)          5325500     flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 500)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            501         dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1)            0           dense_2[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,333,651\n",
      "Trainable params: 5,333,651\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the model parameters\n",
    "batch_size = len(y_scaled) * 0.01 # no bigger than 1 % of data\n",
    "filters = 15\n",
    "# kernel_size\n",
    "strides = 1\n",
    "epochs = 10\n",
    "dropout = 0.1\n",
    "num_layers = 10\n",
    "\n",
    "# Define the tensorboard and checkpointer if desired\n",
    "tb = TensorBoard(log_dir='./logs', \n",
    "                 histogram_freq=3, \n",
    "                 batch_size=batch_size, \n",
    "                 write_graph=True, \n",
    "                 write_grads=True, \n",
    "                 write_images=True)\n",
    "checkpointer = ModelCheckpoint(monitor='val_acc', \n",
    "                               filepath=(CHECKPOINTS_DIR + '1dcnn_onehot.hdf5'), \n",
    "                               verbose=1, \n",
    "                               save_best_only=True)\n",
    "\n",
    "# Define the inputs\n",
    "inputs = Input(shape=(max_sequence_len, 5))\n",
    "layers = []\n",
    "\n",
    "# Build up the layers\n",
    "for i in range(1, num_layers + 1):\n",
    "    layer = Conv1D(filters, (2 * i - 1), strides)(inputs)\n",
    "    layers.append(layer)\n",
    "\n",
    "# Combine the layers\n",
    "combined = Concatenate(axis=1)(layers)\n",
    "\n",
    "# Add some flatten, dense, and dropout layers\n",
    "out = Flatten()(combined)\n",
    "# out = Dropout(dropout)(out)\n",
    "out = Dense(500, activation='sigmoid')(out)\n",
    "out = Dropout(dropout)(out)\n",
    "out = Dense(1, activation='sigmoid')(out)\n",
    "out = Dropout(dropout)(out)\n",
    "\n",
    "# Define the model with inputs and outputs, and compile.\n",
    "model = Model(inputs=inputs, outputs=out)\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "slice indices must be integers or None or have an __index__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-3e7a79df165e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs,verbose=1,\n\u001b[0;32m----> 3\u001b[0;31m                     validation_data=(X_test, y_test))#, callbacks=[checkpointer])\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/yeast/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/miniconda3/envs/yeast/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/miniconda3/envs/yeast/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_end\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0mbatch_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;31m# Slice into a batch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: slice indices must be integers or None or have an __index__ method"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs,verbose=1,\n",
    "                    validation_data=(X_test, y_test))#, callbacks=[checkpointer])\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "score = max(history.history['val_acc'])\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], score*100))\n",
    "plt = construct.plot_results(history.history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yeast",
   "language": "python",
   "name": "yeast"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
