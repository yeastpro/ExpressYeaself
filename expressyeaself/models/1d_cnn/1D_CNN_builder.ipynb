{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the 1D CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import context\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import subprocess\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "# tf.get_variable('test_bool', 1, tf.bool)\n",
    "from tensorflow.python.keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.python.keras.preprocessing import sequence\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.python.keras.layers import (Add, Concatenate, Input, Dense, \n",
    "                                            Dropout, Embedding, Conv1D, \n",
    "                                            MaxPooling1D, GlobalAveragePooling1D, \n",
    "                                            Flatten)\n",
    "from tensorflow.python.keras import regularizers\n",
    "from tensorflow.python.keras.utils import to_categorical\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "ROOT_DIR = os.getcwd() + '/'\n",
    "CHECKPOINTS_DIR = ROOT_DIR + 'expressyeaself/models/1dcnn/checkpoints/'\n",
    "construct = context.construct_neural_net\n",
    "encode = context.encode_sequences\n",
    "organize = context.organize_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the full data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_filename = ('20190612130111781831_percentiles_els_binarized_homogeneous'\n",
    "                   '_deflanked_sequences_with_exp_levels.txt.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a smaller sample set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_filename = '10000_from_' + sample_filename "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the absolute path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = ROOT_DIR + 'example/processed_data/' + sample_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as t\n",
    "t0 = t.time()\n",
    "X_padded, y_scaled, abs_max_el = encode.encode_sequences_with_method(sample_path, method='One-Hot', scale_els=True, model_type='1DCNN', binarized_els=True)\n",
    "num_seqs, max_sequence_len = organize.get_num_and_len_of_seqs_from_file(sample_path)\n",
    "t1 = t.time()\n",
    "print(t1-t0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape expression levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scaled = y_scaled.reshape((len(y_scaled), 1))\n",
    "# scaler = MinMaxScaler()\n",
    "# scaler.fit(y_scaled)\n",
    "# y_scaled = scaler.transform(y_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform a train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_scaled, test_size=test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model parameters\n",
    "batch_size = len(y_scaled) * 0.01 # no bigger than 1 % of data\n",
    "filters = 15\n",
    "kernel_size = 3\n",
    "strides = 1\n",
    "epochs = 20\n",
    "dropout = 0.5\n",
    "\n",
    "# Define the tensorboard and checkpointer if desired\n",
    "tb = TensorBoard(log_dir='./logs', \n",
    "                 histogram_freq=3, \n",
    "                 batch_size=batch_size, \n",
    "                 write_graph=True, \n",
    "                 write_grads=True, \n",
    "                 write_images=True)\n",
    "checkpointer = ModelCheckpoint(monitor='val_acc', \n",
    "                               filepath=(CHECKPOINTS_DIR + '1dcnn_onehot.hdf5'), \n",
    "                               verbose=1, \n",
    "                               save_best_only=True)\n",
    "\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "\n",
    "# Build up the layers\n",
    "model.add(Conv1D(filters, kernel_size, activation='relu', \n",
    "                 input_shape=(max_sequence_len, 5), \n",
    "                 kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "model.add(MaxPooling1D(3, strides))\n",
    "#     keras.layers.Flatten(data_format=None)\n",
    "# model.add(GlobalAveragePooling1D())\n",
    "model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "model.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "\n",
    "# Add some dense and dropout layers\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mse', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model parameters\n",
    "batch_size = len(y_scaled) * 0.01 # no bigger than 1 % of data\n",
    "filters = 15\n",
    "# kernel_size\n",
    "strides = 1\n",
    "epochs = 10\n",
    "dropout = 0.1\n",
    "num_layers = 10\n",
    "\n",
    "# Define the tensorboard and checkpointer if desired\n",
    "tb = TensorBoard(log_dir='./logs', \n",
    "                 histogram_freq=3, \n",
    "                 batch_size=batch_size, \n",
    "                 write_graph=True, \n",
    "                 write_grads=True, \n",
    "                 write_images=True)\n",
    "checkpointer = ModelCheckpoint(monitor='val_acc', \n",
    "                               filepath=(CHECKPOINTS_DIR + '1dcnn_onehot.hdf5'), \n",
    "                               verbose=1, \n",
    "                               save_best_only=True)\n",
    "\n",
    "# Define the inputs\n",
    "inputs = Input(shape=(max_sequence_len, 5))\n",
    "layers = []\n",
    "\n",
    "# Build up the layers\n",
    "for i in range(1, num_layers + 1):\n",
    "    layer = Conv1D(filters, (2 * i - 1), strides)(inputs)\n",
    "    layers.append(layer)\n",
    "\n",
    "# Combine the layers\n",
    "combined = Concatenate(axis=1)(layers)\n",
    "\n",
    "# Add some flatten, dense, and dropout layers\n",
    "out = Flatten()(combined)\n",
    "# out = Dropout(dropout)(out)\n",
    "out = Dense(500, activation='sigmoid')(out)\n",
    "out = Dropout(dropout)(out)\n",
    "out = Dense(1, activation='sigmoid')(out)\n",
    "out = Dropout(dropout)(out)\n",
    "\n",
    "# Define the model with inputs and outputs, and compile.\n",
    "model = Model(inputs=inputs, outputs=out)\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit and Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs,verbose=1,\n",
    "                    validation_data=(X_test, y_test))#, callbacks=[checkpointer])\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "score = max(history.history['val_acc'])\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], score*100))\n",
    "plt = construct.plot_results(history.history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yeast",
   "language": "python",
   "name": "yeast"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
